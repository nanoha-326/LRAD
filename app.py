import streamlit as st
import openai
import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import os, re, unicodedata, base64

# â”€â”€ ãƒšãƒ¼ã‚¸è¨­å®š â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(page_title="LRADã‚µãƒãƒ¼ãƒˆãƒãƒ£ãƒƒãƒˆ", layout="centered")
openai.api_key = st.secrets.OpenAIAPI.openai_api_key

# â”€â”€ CSSï¼ˆæœ¬æ–‡ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚ºã ã‘å¯å¤‰ï¼‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def inject_custom_css(px: str):
    st.markdown(f"""
    <style>
    .chat-text, .stCaption, .stTextInput > label {{font-size:{px}!important}}
    .stTextInput input {{font-size:{px}!important}}
    ::placeholder {{font-size:{px}!important}}
    </style>""", unsafe_allow_html=True)

# â”€â”€ ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def is_valid_input(text:str)->bool:
    text=text.strip()
    if not (3<=len(text)<=300): return False
    if len(re.findall(r'[^A-Za-z0-9ã-ã‚“ã‚¡-ãƒ¶ä¸€-é¾ \\s]',text))/len(text)>0.3: return False
    try: unicodedata.normalize("NFKC",text).encode("utf-8")
    except UnicodeError: return False
    return True

def get_embedding(t, model="text-embedding-3-small"):
    t=t.replace("\n"," ")
    return np.array(openai.embeddings.create(input=[t], model=model).data[0].embedding)

@st.cache_data(show_spinner=False)
def load_faq_all(path="faq_all.csv", cached="faq_all_with_embed.csv"):
    if os.path.exists(cached):
        df=pd.read_csv(cached)
        df["embedding"]=df["embedding"].apply(eval).apply(np.array)
    else:
        df=pd.read_csv(path)
        with st.spinner("FAQåŸ‹ã‚è¾¼ã¿è¨ˆç®—ä¸­â€¦"):
            df["embedding"]=df["è³ªå•"].apply(get_embedding)
        df.to_csv(cached,index=False)
    return df

@st.cache_data(show_spinner=False)
def load_faq_common(path="faq_common.csv"):
    df=pd.read_csv(path,encoding="utf-8-sig"); df.columns=df.columns.str.strip(); return df

faq_df=load_faq_all()
common_faq_df=load_faq_common()

def find_top_similar(q):
    q_vec=get_embedding(q)
    mat=np.stack(faq_df["embedding"].to_numpy())
    sims=np.dot(mat, q_vec)/(np.linalg.norm(mat,1)+1e-9)
    idx=sims.argmax()
    return faq_df.iloc[idx]["è³ªå•"], faq_df.iloc[idx]["å›ç­”"]

def gpt_chat(messages):
    return openai.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=messages,
        temperature=0.3
    ).choices[0].message.content.strip()

# â”€â”€ ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if "chat_log" not in st.session_state: st.session_state.chat_log=[]
if "summary"   not in st.session_state: st.session_state.summary=""

# â”€â”€ ã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®š â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.sidebar.title("âš™ï¸ è¡¨ç¤ºè¨­å®š")
size_choice=st.sidebar.selectbox("æœ¬æ–‡ã‚µã‚¤ã‚º",["å°","ä¸­","å¤§"])
size_map={"å°":"14px","ä¸­":"18px","å¤§":"24px"}
img_map={"å°":60,"ä¸­":80,"å¤§":110}
inject_custom_css(size_map[size_choice])

# â”€â”€ ãƒ˜ãƒƒãƒ€ãƒ¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def img_b64(path): return base64.b64encode(open(path,"rb").read()).decode()
header_img=img_b64("LRADimg.png")
st.markdown(f"""
<div style="display:flex;align-items:center">
  <img src="data:image/png;base64,{header_img}" width="{img_map[size_choice]}" style="margin-right:10px;">
  <h1 style="margin:0;font-size:40px;font-weight:bold;">LRADã‚µãƒãƒ¼ãƒˆãƒãƒ£ãƒƒãƒˆ</h1>
</div>""", unsafe_allow_html=True)
st.caption("â€»ã“ã®ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã¯FAQã¨AIã‚’ã‚‚ã¨ã«å¿œç­”ã—ã¾ã™ãŒã€ã™ã¹ã¦ã®è³ªå•ã«æ­£ç¢ºã«å›ç­”ã§ãã‚‹ã¨ã¯é™ã‚Šã¾ã›ã‚“ã€‚")

# â”€â”€ ã‚ˆãã‚ã‚‹è³ªå•ãƒ©ãƒ³ãƒ€ãƒ è¡¨ç¤º â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.markdown("### ğŸ’¡ ã‚ˆãã‚ã‚‹è³ªå•ï¼ˆãƒ©ãƒ³ãƒ€ãƒ è¡¨ç¤ºï¼‰")
for _,r in common_faq_df.sample(3).iterrows():
    st.markdown(f'<div class="chat-text"><b>â“ {r.è³ªå•}</b><br>ğŸ…°ï¸ {r.å›ç­”}</div><hr>',unsafe_allow_html=True)

st.divider()

# â”€â”€ å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with st.form("chat_form", clear_on_submit=True):
    user_q=st.text_input("è³ªå•ã‚’ã©ã†ãï¼š")
    send=st.form_submit_button("é€ä¿¡")

# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
# â”‚ ãƒ¡ãƒ¢ãƒªæ–¹å¼: è¦ç´„ï¼‹ç›´è¿‘ãƒ­ã‚° ï¼ˆ6ã‚¿ãƒ¼ãƒ³ã§è¦ç´„ï¼‰ â”‚
# â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
def update_memory():
    """6ã‚¿ãƒ¼ãƒ³è¶…ãˆãŸã‚‰å¤ã„4ã‚¿ãƒ¼ãƒ³ã‚’è¦ç´„ã— summary ã«è¿½è¨˜"""
    if len(st.session_state.chat_log) <= 6: return
    older = st.session_state.chat_log[2:]  # å¤ã„åˆ†ï¼ˆå…ˆé ­=æœ€æ–°ï¼‰
    # è¦ç´„ç”¨ãƒ†ã‚­ã‚¹ãƒˆ
    history_text = "\n".join([f"ãƒ¦ãƒ¼ã‚¶ãƒ¼:{q}\nAI:{a}" for q,a in older[::-1]])
    summary_prompt = [
        {"role":"system","content":"ä»¥ä¸‹ã®ä¼šè©±ã‚’æ—¥æœ¬èªã§100å­—ä»¥å†…ã«è¦ç´„ã—ã¦ãã ã•ã„ã€‚"},
        {"role":"user","content":history_text}
    ]
    summary = gpt_chat(summary_prompt)
    st.session_state.summary += "\n" + summary
    # å¤ã„ãƒ­ã‚°ã‚’æ¨ã¦ã¦æœ€æ–°2ã‚¿ãƒ¼ãƒ³ã ã‘æ®‹ã™
    st.session_state.chat_log = st.session_state.chat_log[:2]

if send and user_q:
    if not is_valid_input(user_q):
        st.warning("å…¥åŠ›ãŒä¸æ­£ã§ã™ã€‚3ã€œ300æ–‡å­—ã€è¨˜å·ç‡30%æœªæº€ã«ã—ã¦ãã ã•ã„ã€‚")
    else:
        faq_q, faq_a = find_top_similar(user_q)
        # ä¼šè©±ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’çµ„ã¿ç«‹ã¦
        sys_msg = {"role":"system","content":
            "ã‚ãªãŸã¯LRADï¼ˆé èµ¤å¤–ç·šé›»å­ç†±åˆ†è§£è£…ç½®ï¼‰ã®å°‚é–€å®¶ã§ã™ã€‚ä¸å¯§ã«ç­”ãˆã¦ãã ã•ã„ã€‚"}
        mem_msg = {"role":"system","content":f"ã“ã‚Œã¾ã§ã®ä¼šè©±è¦ç´„:\n{st.session_state.summary}"} if st.session_state.summary else None
        history_msgs = []
        for q,a in st.session_state.chat_log[:2][::-1]:  # ç›´è¿‘2ã‚¿ãƒ¼ãƒ³ã®ã¿
            history_msgs.extend([{"role":"user","content":q},{"role":"assistant","content":a}])
        user_context = {"role":"user","content":
            f"ãƒ¦ãƒ¼ã‚¶ãƒ¼è³ªå•:{user_q}\nå‚è€ƒFAQè³ªå•:{faq_q}\nå‚è€ƒFAQå›ç­”:{faq_a}"}

        msgs = [sys_msg] + ([mem_msg] if mem_msg else []) + history_msgs + [user_context]
        answer = gpt_chat(msgs)

        st.session_state.chat_log.insert(0,(user_q,answer))
        update_memory()
        st.experimental_rerun()

# â”€â”€ ãƒãƒ£ãƒƒãƒˆå±¥æ­´è¡¨ç¤º â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if st.session_state.chat_log:
    st.subheader("ğŸ“œ ãƒãƒ£ãƒƒãƒˆå±¥æ­´ï¼ˆæœ€æ–°â†’å¤ã„ï¼‰")
    for q,a in st.session_state.chat_log:
        st.markdown(f'<div class="chat-text"><b>ğŸ§‘â€ğŸ’» è³ªå•:</b> {q}<br><b>ğŸ¤– å›ç­”:</b> {a}</div><hr>',unsafe_allow_html=True)
